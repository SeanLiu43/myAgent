from dotenv import load_dotenv
load_dotenv()

import warnings
warnings.filterwarnings("ignore")

# pip install langchain langchain-anthropic langchain-community
# pip install chromadb       # å‘é‡æ•°æ®åº“ï¼ˆæœ¬åœ°å…è´¹ï¼‰
# pip install langchain-huggingface  # Embeddingæ¨¡å‹
# pip install -r requirements.txt
# pip install sentence-transformers


from langchain_anthropic import ChatAnthropic
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.vectorstores import FAISS
# pip install faiss-cpu
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# ========== 1. åŠ è½½æ–‡æ¡£ ==========
print("ğŸ“„ åŠ è½½æ–‡æ¡£...")
loader = DirectoryLoader("docs", glob="**/*.txt", loader_cls=TextLoader)
documents = loader.load()
print(f"   åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")

# ========== 2. æ–‡æ¡£åˆ‡ç‰‡ ==========
print("âœ‚ï¸ åˆ‡åˆ†æ–‡æ¡£...")
splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,      # æ¯ä¸ªç‰‡æ®µæœ€å¤§200å­—ç¬¦
    chunk_overlap=50     # ç‰‡æ®µä¹‹é—´é‡å 50å­—ç¬¦ï¼Œä¿æŒä¸Šä¸‹æ–‡
)
chunks = splitter.split_documents(documents)
print(f"   åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")

# ========== 3. å‘é‡åŒ–å¹¶å­˜å…¥å‘é‡æ•°æ®åº“ ==========
print("ğŸ”¢ å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“...")
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
# vectorstore = Chroma.from_documents(
#     documents=chunks,
#     embedding=embeddings,
    # persist_directory="./chroma_db"  # æŒä¹…åŒ–åˆ°æœ¬åœ°
# )

vectorstore = FAISS.from_documents(
    documents=chunks,
    embedding=embeddings
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # æ¯æ¬¡æ£€ç´¢3ä¸ªæœ€ç›¸å…³ç‰‡æ®µ
print("   âœ… å‘é‡æ•°æ®åº“åˆ›å»ºå®Œæˆ")

# ========== 4. æ„å»º RAG Chain ==========
llm = ChatAnthropic(model_name="claude-sonnet-4-20250514")

prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®å‘Šè¯‰ç”¨æˆ·ä½ ä¸çŸ¥é“ï¼Œä¸è¦ç¼–é€ ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}"""),
    ("human", "{question}")
])

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
)

# ========== 5. å¯¹è¯å¾ªç¯ ==========
print("\nğŸ¤– RAG çŸ¥è¯†åº“åŠ©æ‰‹å·²å¯åŠ¨ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰\n")

while True:
    question = input("ä½ : ")
    if question.lower() in ["quit", "exit"]:
        break

    response = rag_chain.invoke(question)
    print(f"AI: {response.content}\n")