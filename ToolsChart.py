from dotenv import load_dotenv
load_dotenv()

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.tools import tool

# ========== 1. å®šä¹‰å·¥å…· ==========
@tool
def search(query: str) -> str:
    """æœç´¢äº’è”ç½‘ä¸Šçš„ä¿¡æ¯ã€‚å½“ç”¨æˆ·è¯¢é—®ä½ ä¸çŸ¥é“çš„å®æ—¶ä¿¡æ¯æ—¶ä½¿ç”¨ã€‚"""
    # è¿™é‡Œæ˜¯æ¨¡æ‹Ÿæœç´¢ï¼Œå®é™…å¯ä»¥æ¥å…¥æœç´¢API
    return f"æœç´¢ç»“æœï¼šå…³äº'{query}'çš„æœ€æ–°ä¿¡æ¯æ˜¯..."

@tool
def calculator(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚å½“ç”¨æˆ·éœ€è¦åšæ•°å­¦è¿ç®—æ—¶ä½¿ç”¨ã€‚"""
    try:
        result = eval(expression)
        return f"è®¡ç®—ç»“æœï¼š{expression} = {result}"
    except Exception as e:
        return f"è®¡ç®—é”™è¯¯ï¼š{e}"

# ========== 2. åˆ›å»ºå¸¦å·¥å…·çš„ LLM ==========
tools = [search, calculator]
llm = ChatAnthropic(model_name="claude-sonnet-4-20250514")
llm_with_tools = llm.bind_tools(tools)

# ========== 3. æ„å»º Chain ==========
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·æ¥å¸®åŠ©ç”¨æˆ·ã€‚"),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

chain = prompt | llm_with_tools

# ========== 4. å·¥å…·æ‰§è¡Œå‡½æ•° ==========
tool_map = {"search": search, "calculator": calculator}

def process_response(response):
    """å¦‚æœ AI è¦è°ƒç”¨å·¥å…·ï¼Œå°±æ‰§è¡Œå·¥å…·å¹¶è¿”å›æœ€ç»ˆç»“æœ"""
    if response.tool_calls:
        print(f"  ğŸ”§ AI æ­£åœ¨è°ƒç”¨å·¥å…·...")
        messages = [response]  # å…ˆæ”¾å…¥ AI çš„å›å¤

        for tc in response.tool_calls:
            tool_name = tc["name"]
            tool_args = tc["args"]
            print(f"  ğŸ”§ è°ƒç”¨: {tool_name}({tool_args})")

            # æ‰§è¡Œå·¥å…·
            result = tool_map[tool_name].invoke(tool_args)
            print(f"  ğŸ“ ç»“æœ: {result}")

            # æŠŠå·¥å…·ç»“æœåŒ…è£…æˆ ToolMessage
            from langchain_core.messages import ToolMessage
            messages.append(ToolMessage(content=result, tool_call_id=tc["id"]))

        # æŠŠå·¥å…·ç»“æœå‘å›ç»™ LLMï¼Œè®©å®ƒç”Ÿæˆæœ€ç»ˆå›ç­”
        final_response = llm_with_tools.invoke(messages)
        return final_response.content
    else:
        return response.content

# ========== 5. å¯¹è¯å¾ªç¯ ==========
history = []

print("èŠå¤©æœºå™¨äººå·²å¯åŠ¨ï¼ˆè¾“å…¥ quit é€€å‡ºï¼‰")
print("æˆ‘å¯ä»¥å¸®ä½ æœç´¢ä¿¡æ¯å’Œåšæ•°å­¦è®¡ç®—ï¼\n")

while True:
    user_input = input("ä½ : ")
    if user_input.lower() in ["quit", "exit"]:
        break

    response = chain.invoke({"history": history, "input": user_input})
    answer = process_response(response)
    print(f"AI: {answer}\n")

    history.append(HumanMessage(content=user_input))
    history.append(AIMessage(content=answer))

#  Agent çš„æ ¸å¿ƒæ€æƒ³ï¼šLLM ä½œä¸º"å¤§è„‘"ï¼Œè‡ªå·±å†³å®šç”¨ä¸ç”¨å·¥å…·ã€ç”¨å“ªä¸ªå·¥å…·ã€æ€ä¹ˆç”¨ç»“æœ